{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Anomaly Detection\n",
    "\n",
    "Baseline from: https://arxiv.org/pdf/2006.02516.pdf\n",
    "\n",
    "- 14x14, [$0-1$] range, pixels flattened (dont exploit inherent locality)\n",
    "- Embedding = trigonometric (p=2)\n",
    "- Combined loss: $\\mathcal{L} = \\frac{1}{N}\\sum_{i=1}^{N} \\left( \\log \\left\\| P \\Phi({X_i}) \\right\\|_2^2 - 1 \\right)^2 + \\alpha \\cdot \\mathrm{ReLU}\\left(\\log(\\|P\\|_F^2)\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbsphinx": {
     "title": "# Anomaly detection MNIST - SMPO"
    }
   },
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import quimb.tensor as qtn\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from jax.nn.initializers import *\n",
    "from flax.training.early_stopping import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tn4ml.initializers import *\n",
    "from tn4ml.models.smpo import *\n",
    "from tn4ml.models.model import *\n",
    "from tn4ml.embeddings import *\n",
    "from tn4ml.metrics import *\n",
    "from tn4ml.util import *\n",
    "from tn4ml.eval import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.config.update(\"jax_enable_x64\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = mnist.load_data()\n",
    "data = {\"X\": dict(train=train[0], test=test[0]), \"y\": dict(train=train[1], test=test[1])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_class = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "X = {\n",
    "\"normal\": data[\"X\"][\"train\"][data[\"y\"][\"train\"] == normal_class]/255.0,\n",
    "\"anomaly\": data[\"X\"][\"train\"][data[\"y\"][\"train\"] != normal_class]/255.0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data\n",
    "X_test = {\n",
    "\"normal\": data[\"X\"][\"test\"][data[\"y\"][\"test\"] == normal_class]/255.0,\n",
    "\"anomaly\": data[\"X\"][\"test\"][data[\"y\"][\"test\"] != normal_class]/255.0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to resize images to 14x14\n",
    "def resize_images(images):\n",
    "    resized_images = tf.image.resize(images, [14, 14], method=tf.image.ResizeMethod.AREA)\n",
    "    return resized_images.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_resized = {\n",
    "    \"normal\": resize_images(np.expand_dims(X[\"normal\"], axis=-1)),\n",
    "    \"anomaly\": resize_images(np.expand_dims(X[\"anomaly\"], axis=-1)),\n",
    "}\n",
    "\n",
    "X_test_resized = {\n",
    "    \"normal\": resize_images(np.expand_dims(X_test[\"normal\"], axis=-1)),\n",
    "    \"anomaly\": resize_images(np.expand_dims(X_test[\"anomaly\"], axis=-1)),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rearrange pixels in zig-zag order\n",
    "- (from https://arxiv.org/pdf/1605.05775.pdf)\n",
    "\n",
    "<img src=\"images/zig-zag.png\" alt=\"MPS Params\" width=\"150\" height=\"150\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zigzag_order(data):\n",
    "    data = np.squeeze(data)\n",
    "    data_zigzag = []\n",
    "    for x in data:\n",
    "        image = []\n",
    "        for i in x:\n",
    "            image.extend(i)\n",
    "        data_zigzag.append(image)\n",
    "    return np.asarray(data_zigzag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "zigzag = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if zigzag:\n",
    "    train_normal = zigzag_order(X_resized[\"normal\"])\n",
    "    test_normal = zigzag_order(X_test_resized[\"normal\"])\n",
    "\n",
    "    train_anomaly = zigzag_order(X_resized[\"anomaly\"])\n",
    "    test_anomaly = zigzag_order(X_test_resized[\"anomaly\"])\n",
    "else:\n",
    "    train_normal = X_resized['normal'].reshape(-1, X_resized['normal'].shape[1]*X_resized['normal'].shape[2])\n",
    "    test_normal = X_test_resized['normal'].reshape(-1, X_test_resized['normal'].shape[1]*X_test_resized['normal'].shape[2])\n",
    "\n",
    "    train_anomaly = X_resized['anomaly'].reshape(-1, X_resized['anomaly'].shape[1]*X_resized['anomaly'].shape[2])\n",
    "    test_anomaly = X_test_resized['anomaly'].reshape(-1, X_test_resized['anomaly'].shape[1]*X_test_resized['anomaly'].shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take train_size samples from normal class for training\n",
    "train_size = 2048\n",
    "\n",
    "indices = list(range(len(train_normal)))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "indices = indices[:train_size]\n",
    "train_normal = np.take(train_normal, indices, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training setup** &nbsp;\n",
    "- direct gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model parameters\n",
    "L = 14*14\n",
    "initializer = gramschmidt('normal', 1e-2)\n",
    "key = jax.random.key(42)\n",
    "shape_method = 'noteven'\n",
    "bond_dim = 10\n",
    "phys_dim = (2,2)\n",
    "spacing = 8\n",
    "add_identity = False\n",
    "boundary='obc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SMPO_initialize(L=L,\n",
    "                        initializer=initializer,\n",
    "                        key=key,\n",
    "                        shape_method=shape_method,\n",
    "                        spacing=spacing,\n",
    "                        bond_dim=bond_dim,\n",
    "                        phys_dim=phys_dim,\n",
    "                        cyclic=False,\n",
    "                        compress=True,\n",
    "                        add_identity=add_identity,\n",
    "                        boundary=boundary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Exponential decay of the learning rate.\n",
    "# scheduler = optax.exponential_decay(\n",
    "#     init_value=1e-5,\n",
    "#     transition_steps=1000,\n",
    "#     decay_rate=0.01)\n",
    "\n",
    "# # Combining gradient transforms using `optax.chain`.\n",
    "# gradient_transforms = [\n",
    "#     optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "#     optax.scale_by_adam(),  # Use the updates from adam.\n",
    "#     optax.scale_by_schedule(scheduler),  # Use the learning rate from the scheduler.\n",
    "#     # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "#     optax.scale(-1.0)\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.4\n",
    "def loss_combined(*args, **kwargs):\n",
    "    error = LogQuadNorm\n",
    "    reg = LogReLUFrobNorm\n",
    "    return CombinedLoss(*args, **kwargs, error=error, reg=lambda P: alpha*reg(P))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training parameters\n",
    "epochs = 100\n",
    "batch_size = 128\n",
    "optimizer = optax.adam\n",
    "strategy = 'global'\n",
    "loss = loss_combined\n",
    "train_type = 0\n",
    "embedding = trigonometric()\n",
    "learning_rate = 1e-4\n",
    "earlystop = EarlyStopping(min_delta=0, patience=5, monitor='loss', mode='min')\n",
    "\n",
    "model.configure(optimizer=optimizer, strategy=strategy, loss=loss, train_type=train_type, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.train(train_normal,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            embedding = embedding,\n",
    "            normalize=True,\n",
    "            earlystop=earlystop,\n",
    "            cache=True, # compiling loss function to use it multiple times\n",
    "            dtype=jnp.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss\n",
    "plot_loss(history, validation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.loglog(range(len(model.history['loss'])), model.history['loss'], label='train')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = list(range(len(test_anomaly)))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "indices = indices[:len(test_normal)]\n",
    "test_anomaly = np.take(test_anomaly, indices, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = LogQuadNorm\n",
    "\n",
    "anomaly_score = model.evaluate(test_anomaly, evaluate_type=0, return_list=True, dtype=jnp.float64, batch_size=128, embedding=embedding, metric = loss)\n",
    "normal_score = model.evaluate(test_normal, evaluate_type=0, return_list=True, dtype=jnp.float64, batch_size=128, embedding=embedding, metric = loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot anomaly scores and ROC curve**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr = get_roc_curve_data(normal_score, anomaly_score)\n",
    "auc_value = auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(anomaly_score, bins=100, histtype='step', label='anomaly')\n",
    "plt.hist(normal_score, bins=100, histtype='step', label='normal')\n",
    "plt.title('Anomaly score distribution')\n",
    "plt.legend()\n",
    "plt.text(0.5, -0.1, f'AUC Value: {auc_value}', ha='center', transform=plt.gca().transAxes)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curve\n",
    "plot_ROC_curve_from_data(fpr, tpr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
