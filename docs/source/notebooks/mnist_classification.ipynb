{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbsphinx": {
     "title": "First steps"
    }
   },
   "source": [
    "# MNIST classification with TNs\n",
    "\n",
    "- 14x14, [$0-1$] range, zig-zag pixel ordering\n",
    "- embedding = trigonometric\n",
    "- sweeping strategy (claim that 2-3 sweeps is enough)\n",
    "- quadratic cost\n",
    "- bond_dim = 10, 20, 120\n",
    "- MPS (with output dim = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from jax.nn.initializers import *\n",
    "\n",
    "from tn4ml.initializers import *\n",
    "from tn4ml.models.smpo import *\n",
    "from tn4ml.models.model import *\n",
    "from tn4ml.embeddings import *\n",
    "from tn4ml.loss import *\n",
    "from tn4ml.strategy import *\n",
    "from tn4ml.util import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"X\": dict(train=train[0], test=test[0]), \"y\": dict(train=train[1], test=test[1])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce the size of the image\n",
    "strides = (2,2) # (2,2) for 14x14 images; (4,4) for 7x7 images\n",
    "pool_size = (2,2)\n",
    "pool = tf.keras.layers.MaxPooling2D(pool_size=pool_size, strides=strides, padding=\"same\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pooled = pool(tf.constant(data['X']['train'].reshape(-1,28,28,1))).numpy().reshape(-1,14,14)/255.0\n",
    "X_pooled_test = pool(tf.constant(data['X']['test'].reshape(-1,28,28,1))).numpy().reshape(-1,14,14)/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rearagne pixels in zig-zag order (from https://arxiv.org/pdf/1605.05775.pdf)\n",
    "\n",
    "def zigzag_order(data):\n",
    "    data_zigzag = []\n",
    "    for x in data:\n",
    "        image = []\n",
    "        for i in x:\n",
    "            image.extend(i)\n",
    "        data_zigzag.append(image)\n",
    "    return np.asarray(data_zigzag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = zigzag_order(X_pooled)\n",
    "test_data = zigzag_order(X_pooled_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = integer_to_one_hot(data['y']['train'], n_classes)\n",
    "y_test = integer_to_one_hot(data['y']['test'], n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Take samples for training, validation and testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 6000\n",
    "test_size = 10000\n",
    "val_perc = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take val_size samples from normal class for validation (X% of training data)\n",
    "val_size = int(val_perc*train_size)\n",
    "train_size = int(train_size - val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_size, train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = list(range(len(train_data)))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "train_indices = indices[:train_size]\n",
    "val_indices = indices[train_size : train_size+val_size]\n",
    "\n",
    "# train data and validation inputs\n",
    "train_inputs = np.take(train_data, train_indices, axis=0)\n",
    "val_inputs = np.take(train_data, val_indices, axis=0)\n",
    "\n",
    "\n",
    "# train data and validation labels\n",
    "train_targets = np.take(y_train, train_indices, axis=0)\n",
    "val_targets = np.take(y_train, val_indices, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = list(range(len(test_data)))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "test_indices = indices[:test_size]\n",
    "\n",
    "# test inputs\n",
    "test_inputs = np.take(test_data, test_indices, axis=0)\n",
    "\n",
    "# test labels\n",
    "test_targets = np.take(y_test, test_indices, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training setup** &nbsp;\n",
    "- direct gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "L = 196\n",
    "initializer = jax.nn.initializers.normal(0.5)\n",
    "key = jax.random.key(42)\n",
    "shape_method = 'noteven'\n",
    "bond_dim = 10\n",
    "phys_dim = (2, n_classes)\n",
    "spacing = L\n",
    "canonical_center=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SMPO_initialize(L=L,\n",
    "                        initializer=initializer,\n",
    "                        key=key,\n",
    "                        shape_method=shape_method,\n",
    "                        spacing=spacing,\n",
    "                        bond_dim=bond_dim,\n",
    "                        phys_dim=phys_dim,\n",
    "                        cyclic=False,\n",
    "                        canonical_center=canonical_center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE_loss(*args, **kwargs):\n",
    "    return loss_wrapper_optax(optax.squared_error)(*args, **kwargs).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When using cross-entropy loss I need to put reduce_mean**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters\n",
    "optimizer = optax.adam\n",
    "strategy = 'global'\n",
    "loss = MSE\n",
    "train_type = 1\n",
    "#embedding = original_inverse(p=3)\n",
    "embedding = trigonometric()\n",
    "learning_rate = 5e-5\n",
    "\n",
    "# Exponential decay of the learning rate.\n",
    "scheduler = optax.exponential_decay(\n",
    "    init_value=1e-4,\n",
    "    transition_steps=1000,\n",
    "    decay_rate=0.01)\n",
    "\n",
    "# Combining gradient transforms using `optax.chain`.\n",
    "gradient_transforms = [\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    optax.scale_by_schedule(scheduler),  # Use the learning rate from the scheduler.\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.configure(optimizer=optimizer, strategy=strategy, loss=loss, train_type=train_type, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.train(train_inputs,\n",
    "                    targets = train_targets,\n",
    "                    val_inputs = val_inputs,\n",
    "                    val_targets = val_targets,\n",
    "                    epochs = epochs,\n",
    "                    batch_size = batch_size,\n",
    "                    embedding = embedding,\n",
    "                    normalize = True,\n",
    "                    cache=True,\n",
    "                    dtype = jnp.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.configure(gradient_transforms=gradient_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 280"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.train(train_inputs,\n",
    "                    targets = train_targets,\n",
    "                    val_inputs = val_inputs,\n",
    "                    val_targets = val_targets,\n",
    "                    epochs = epochs,\n",
    "                    batch_size = batch_size,\n",
    "                    embedding = embedding,\n",
    "                    normalize = True,\n",
    "                    cache=True,\n",
    "                    dtype = jnp.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# plot loss\n",
    "plt.plot(range(len(model.history['loss'])), model.history['loss'], label='train')\n",
    "plt.plot(range(len(model.history['val_loss'])), model.history['val_loss'], label='validation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "# model.save('model', 'tests/mnist_supervised_model6')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tn4ml.models.model import _batch_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "correct_predictions = 0; total_loss = 0\n",
    "\n",
    "for batch_data in _batch_iterator(test_inputs, test_targets, batch_size=batch_size):\n",
    "    x, y = batch_data\n",
    "    x = jnp.array(x, dtype=jnp.float64)\n",
    "    y = jnp.array(y, dtype=jnp.float64)\n",
    "\n",
    "    y_pred = jnp.squeeze(jnp.array(jax.vmap(model.predict, in_axes=(0, None, None))(x, embedding, False)[0]))\n",
    "    predicted = jnp.argmax(y_pred, axis=-1)\n",
    "    true = jnp.argmax(y, axis=-1)\n",
    "\n",
    "    correct_predictions += jnp.sum(predicted == true).item() / batch_size\n",
    "\n",
    "accuracy = correct_predictions / (len(test_targets)//batch_size)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
