{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbsphinx": {
     "title": "# Classification for Bars-Stripes dataset"
    }
   },
   "source": [
    "# Classification for Bars-Stripes dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow tutorial from $\\rightarrow$ https://pennylane.ai/qml/demos/tutorial_tn_circuits/\n",
    "- check on images 4x4, 8x8, 16x16\n",
    "- on MPS - acc = 50%\n",
    "- check on TTNs - **TODO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.nn.initializers import *\n",
    "import optax\n",
    "from tn4ml.embeddings import *\n",
    "from tn4ml.util import *\n",
    "from tn4ml.models.model import *\n",
    "from tn4ml.models.smpo import *\n",
    "from tn4ml.initializers import *\n",
    "from tn4ml.loss import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bars_and_stripes(n_samples, height, width, noise_std):\n",
    "    \"\"\"Data generation procedure for 'bars and stripes'.\n",
    "\n",
    "    Args:\n",
    "        n_samples (int): number of data samples to produce\n",
    "        height (int): number of pixels for image height\n",
    "        width (int): number of pixels for image width\n",
    "        noise_std (float): standard deviation of Gaussian noise added to the pixels\n",
    "    \"\"\"\n",
    "    X = np.ones([n_samples, 1, height, width]) * -1\n",
    "    y = []\n",
    "\n",
    "    for i in range(len(X)):\n",
    "        if np.random.rand() > 0.5:\n",
    "            rows = np.where(np.random.rand(width) > 0.5)[0]\n",
    "            X[i, 0, rows, :] = 1.0\n",
    "            y.append([0, 1])\n",
    "        else:\n",
    "            columns = np.where(np.random.rand(height) > 0.5)[0]\n",
    "            X[i, 0, :, columns] = 1.0\n",
    "            y.append([1, 0])\n",
    "        X[i, 0] = X[i, 0] + np.random.normal(0, noise_std, size=X[i, 0].shape)\n",
    "\n",
    "    return X, np.array(y)\n",
    "\n",
    "train_images, train_labels = generate_bars_and_stripes(1000, 4, 4, 0.5)\n",
    "test_images, test_labels = generate_bars_and_stripes(200, 4, 4, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = np.squeeze(train_images)\n",
    "test_images = np.squeeze(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=4, figsize=(8,8))\n",
    "\n",
    "axes[0].imshow(np.reshape(-train_images[0], (4,4)), cmap='gray')\n",
    "axes[1].imshow(np.reshape(-train_images[4], (4,4)), cmap='gray')\n",
    "axes[2].imshow(np.reshape(-train_images[6], (4,4)), cmap='gray')\n",
    "axes[3].imshow(np.reshape(-train_images[3], (4,4)), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate N samples\n",
    "# N = 1000  # For example, generate 10 samples\n",
    "# n_classes = 2\n",
    "# train_size = int(N * 0.9)\n",
    "# # train\n",
    "# #train_data = sample_bars_and_stripes(int(N*0.9)) # 90% of the data for training\n",
    "# # train_images = np.array([data[0] for data in train_data])\n",
    "# # train_labels = np.array([data[1] for data in train_data])\n",
    "# train_data = \n",
    "\n",
    "# # test\n",
    "# #test_data = sample_bars_and_stripes(N)\n",
    "# test_images = np.array([data[0] for data in test_data])\n",
    "# test_labels = np.array([data[1] for data in test_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_patterns(dataset, num_patterns=10):\n",
    "    fig, axes = plt.subplots(1, num_patterns, figsize=(num_patterns * 2, 2))\n",
    "    for i, ax in enumerate(axes):\n",
    "        pattern = dataset[i]\n",
    "        ax.imshow(pattern, cmap='binary')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "visualize_patterns(train_images, 16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n",
      "/Users/emapuljak/miniforge3/envs/workspace/lib/python3.10/site-packages/cotengra/hyperoptimizers/hyper.py:34: UserWarning: Couldn't import `kahypar` - skipping from default hyper optimizer and using basic `labels` method instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'quimb' has no attribute 'experimental'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mquimb\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mqu\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m ttn \u001b[38;5;241m=\u001b[39m \u001b[43mqu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexperimental\u001b[49m\u001b[38;5;241m.\u001b[39mmerabuilder\u001b[38;5;241m.\u001b[39mTTN_randtree_rand(\u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m5\u001b[39m, phys_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, group_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, iso\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'quimb' has no attribute 'experimental'"
     ]
    }
   ],
   "source": [
    "import quimb as qu\n",
    "\n",
    "ttn = qu.experimental.merabuilder.TTN_randtree_rand(16, 5, phys_dim=2, group_size=2, iso=False, seed=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define TN model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "L = 16\n",
    "initializer = noise_init(1e-2, dtype=jnp.float64)\n",
    "key = jax.random.key(42)\n",
    "shape_method = 'noteven'\n",
    "bond_dim = 4\n",
    "phys_dim = (2, n_classes)\n",
    "spacing = L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SMPO_initialize(L=L,\n",
    "                        initializer=initializer,\n",
    "                        key=key,\n",
    "                        shape_method=shape_method,\n",
    "                        spacing=spacing,\n",
    "                        bond_dim=bond_dim,\n",
    "                        phys_dim=phys_dim,\n",
    "                        cyclic=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters\n",
    "optimizer = optax.adam\n",
    "strategy = 'global'\n",
    "loss = loss_wrapper_optax(optax.softmax_cross_entropy)\n",
    "train_type ='supervised'\n",
    "embedding = basis_quantum_encoding(basis={0: np.array([1, 0]), 1: np.array([0, 1])})\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Exponential decay of the learning rate.\n",
    "scheduler = optax.exponential_decay(\n",
    "    init_value=0.0001,\n",
    "    transition_steps=1000,\n",
    "    decay_rate=0.99)\n",
    "\n",
    "# Combining gradient transforms using `optax.chain`.\n",
    "gradient_transforms = [\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    optax.scale_by_schedule(scheduler),  # Use the learning rate from the scheduler.\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.configure(gradient_transforms=gradient_transforms, strategy=strategy, loss=loss, train_type=train_type, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# early stopping from flax\n",
    "from flax.training.early_stopping import EarlyStopping\n",
    "\n",
    "earlystop = EarlyStopping(min_delta=0, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.train(train_images.reshape(train_images.shape[0], 16),\n",
    "                    targets = train_labels,\n",
    "                    epochs = epochs,\n",
    "                    batch_size = batch_size,\n",
    "                    embedding = embedding,\n",
    "                    earlystop=earlystop,\n",
    "                    normalize = True,\n",
    "\n",
    "                    dtype = jnp.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(range(len(history['loss'])), history['loss'], label='train')\n",
    "#plt.plot(range(len(history['val_loss'])), history['val_loss'], label='val')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "#plt.savefig('tests/mnist_supervised_model6/loss.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tn4ml.models.model import _batch_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "correct_predictions = 0; total_loss = 0\n",
    "\n",
    "for batch_data in _batch_iterator(test_images.reshape(test_images.shape[0], 16), test_labels, batch_size=batch_size):\n",
    "    x, y = batch_data\n",
    "    x = jnp.array(x, dtype=jnp.float64)\n",
    "    y = jnp.array(y)\n",
    "\n",
    "    y_pred = jnp.squeeze(jnp.array(jax.vmap(model.predict, in_axes=(0, None, None))(x, embedding, False)[0]))\n",
    "    y_pred\n",
    "    predicted = jnp.argmax(y_pred, axis=-1)\n",
    "    true = jnp.argmax(y, axis=-1)\n",
    "\n",
    "    correct_predictions += jnp.sum(predicted == true).item() / batch_size\n",
    "\n",
    "accuracy = correct_predictions / (len(test_images)//batch_size)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
