{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xtWX4x9DCF5_"
   },
   "source": [
    "# JAX Quickstart\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google/jax/blob/main/docs/notebooks/quickstart.ipynb)\n",
    "\n",
    "**JAX is NumPy on the CPU, GPU, and TPU, with great automatic differentiation for high-performance machine learning research.**\n",
    "\n",
    "With its updated version of [Autograd](https://github.com/hips/autograd), JAX\n",
    "can automatically differentiate native Python and NumPy code. It can\n",
    "differentiate through a large subset of Python’s features, including loops, ifs,\n",
    "recursion, and closures, and it can even take derivatives of derivatives of\n",
    "derivatives. It supports reverse-mode as well as forward-mode differentiation, and the two can be composed arbitrarily\n",
    "to any order.\n",
    "\n",
    "What’s new is that JAX uses\n",
    "[XLA](https://www.tensorflow.org/xla)\n",
    "to compile and run your NumPy code on accelerators, like GPUs and TPUs.\n",
    "Compilation happens under the hood by default, with library calls getting\n",
    "just-in-time compiled and executed. But JAX even lets you just-in-time compile\n",
    "your own Python functions into XLA-optimized kernels using a one-function API.\n",
    "Compilation and automatic differentiation can be composed arbitrarily, so you\n",
    "can express sophisticated algorithms and get maximal performance without having\n",
    "to leave Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "SY8mDvEvCGqk"
   },
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap\n",
    "from jax import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "IMAgNJaMJwPD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.25       0.19661197 0.10499357]\n"
     ]
    }
   ],
   "source": [
    "def sum_logistic(x):\n",
    "  return jnp.sum(1.0 / (1.0 + jnp.exp(-x)))\n",
    "\n",
    "x_small = jnp.arange(3.)\n",
    "derivative_fn = grad(sum_logistic)\n",
    "print(derivative_fn(x_small))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PtNs881Ohioc"
   },
   "source": [
    "Let's verify with finite differences that our result is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "JXI7_OZuKZVO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.24998187 0.1965761  0.10502338]\n"
     ]
    }
   ],
   "source": [
    "def first_finite_differences(f, x):\n",
    "  eps = 1e-3\n",
    "  return jnp.array([(f(x + eps * v) - f(x - eps * v)) / (2 * eps)\n",
    "                   for v in jnp.eye(len(x))])\n",
    "\n",
    "\n",
    "print(first_finite_differences(sum_logistic, x_small))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q2CUZjOWNZ-3"
   },
   "source": [
    "Taking derivatives is as easy as calling {func}`~jax.grad`. {func}`~jax.grad` and {func}`~jax.jit` compose and can be mixed arbitrarily. In the above example we jitted `sum_logistic` and then took its derivative. We can go further:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "TO4g8ny-OEi4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.035325598\n"
     ]
    }
   ],
   "source": [
    "print(grad(jit(grad(jit(grad(sum_logistic)))))(1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yCJ5feKvhnBJ"
   },
   "source": [
    "For more advanced autodiff, you can use {func}`jax.vjp` for reverse-mode vector-Jacobian products and {func}`jax.jvp` for forward-mode Jacobian-vector products. The two can be composed arbitrarily with one another, and with other JAX transformations. Here's one way to compose them to make a function that efficiently computes full Hessian matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Z-JxbiNyhxEW"
   },
   "outputs": [],
   "source": [
    "from jax import jacfwd, jacrev\n",
    "def hessian(fun):\n",
    "  return jit(jacfwd(jacrev(fun)))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "JAX Quickstart.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "jupytext": {
   "formats": "ipynb,md:myst"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('jax')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "be00496777329938ab9822d21afaa2562f5e5481d09e738bb9b43653f4e19043"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
